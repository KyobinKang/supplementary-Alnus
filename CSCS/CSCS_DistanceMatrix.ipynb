{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate a chemical dissimilarity matrix using the chemical structural compositional similarity metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Jeremy Owen (Jeremy.Owen@vuw.ac.nz) <br>\n",
    "**Edited by**: Madeleine Ernst (mernst@ucsd.edu) <br>\n",
    "**Use case**: Calculate a chemical dissimilarity matrix using the chemical structural compositional similarity metric (CSCS) proposed by and adapted from (Sedio et al. 2017, Sources of variation in foliar secondary chemistry in a tropical forest tree community, Ecology 98, 616-623). This metric compares MS1 feature intensities in between samples and takes into account the feature's strucutral similarity by multiplying with the cosine score of resepctive MS2 spectra.<br>\n",
    "**Input file format**: <br>\n",
    "<ul>\n",
    "<li>**Edges table** (.tsv) retrieved from GNPS output (networkedges_selfloop folder). </li>\n",
    "<li>**Feature table** (.csv) with features in rows and samples in columns. This corresponds to the MZmine output table, selecting only features with associated MS2 data. </li> \n",
    "<li>**Attribute table** (.tsv) file with 2 columns, the first column specifying any character string you want to add to the sample name, and the second column containing your sample names. </li>\n",
    "</ul>\n",
    "**Outputs**: CSCS distance matrix, which can be used as input to create a PCoA plot in Emperor (using CSCS_PCoA_Emperor.ipynb) and dendrogram as .pdf output. <br>\n",
    "**Dependencies**: Python 3, numpy, scipy, matplotlib, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Tue Feb 28 22:38:27 2017\n",
    "\n",
    "@author: SpiffKringle\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of your attribute table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_file = \"Alnus_Metadata.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify a method for calculating the distance for hierarchical cluster analysis (dendrogram output). <br>\n",
    "You can choose between *single*, *average*, *complete*, *weighted* and *centroid*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkage_type = \"complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify if MS1 intensities should be treated as binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "SubDict = {}\n",
    "\n",
    "with open(genus_file, 'r') as F:\n",
    "    for line in F:\n",
    "        name = line.split()[1]\n",
    "        SubDict[name] = name\n",
    "        #subgenus =  line.split()[1]+ \"_\" + line.split()[0].split(\"_\")[1]\n",
    "        #SubDict[name] = subgenus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AF-T': 'AF-T',\n",
       " 'AF-L': 'AF-L',\n",
       " 'AF-B': 'AF-B',\n",
       " 'AF-F': 'AF-F',\n",
       " 'Aj-T': 'Aj-T',\n",
       " 'Aj-L': 'Aj-L',\n",
       " 'Aj-B': 'Aj-B',\n",
       " 'Aj-F': 'Aj-F',\n",
       " 'Ah-T': 'Ah-T',\n",
       " 'Ah-L': 'Ah-L',\n",
       " 'Ah-F': 'Ah-F',\n",
       " 'Ahv-T': 'Ahv-T',\n",
       " 'Ahv-L': 'Ahv-L',\n",
       " 'Ahv-F': 'Ahv-F',\n",
       " 'Ahv-B': 'Ahv-B'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SubDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadNodesFile(NodesFile,remove_start = 1, cutoff = 10,\n",
    "                  blank_name=None, sep = \",\",binary = binary):\n",
    "    \n",
    "    \"\"\"\n",
    "    * reads a nodes.out file from GNPS and returns: \n",
    "        > NameDict - a dictionary of form{columnName:index} \n",
    "        > VectorDict - a dictionary of the form {nodeID:[list of intensities for each sample}\n",
    "    * at this stage intensities are actualy number of scans (roughly proportional)\n",
    "    * Assumes that blanks are labelled \"blank\"\n",
    "    * in cases where sample values/blank values < cutoff, sample values are set to zero\n",
    "    * This removes \"contaminant\" ions that were seen in the blank sample to a comprably high level\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"mod1\")\n",
    "    with open(NodesFile, 'r') as F:\n",
    "        #names = F.readline().split(sep)[remove_start:-remove_end]\n",
    "        names = F.readline().split(sep)[remove_start:]\n",
    "        names[-1] = names[-1].strip()\n",
    "        print(\"analysing the following samples:\")\n",
    "        print(\"***\")\n",
    "        \n",
    "        for name in names:\n",
    "            print(name)\n",
    "        print(\"***\")\n",
    "        NameDict = {name:names.index(name) for name in names}\n",
    "        \n",
    "        if blank_name:\n",
    "            blank = NameDict[blank_name]\n",
    "        \n",
    "        else:\n",
    "            blank = None\n",
    "        VectorDict = {}\n",
    "        \n",
    "        for line in F:\n",
    "            data = line.split(sep)\n",
    "            NodeID = data[0]\n",
    "            vector = data[remove_start:]\n",
    "            \n",
    "            if binary:\n",
    "                CleanVector = [1.0 if float(i) > 1000 else 0 for i in vector ]\n",
    "            \n",
    "            else:\n",
    "                CleanVector = [float(i) for i in vector]\n",
    "            VectorDict[NodeID] = CleanVector\n",
    "    \n",
    "    return (NameDict,VectorDict)\n",
    "    # the output of ReadNodesFile is a tuple, consisting of 2 elements, the first element RnF[0], consists\n",
    "    # of a dictionary with length 43, each with the sample name and associated sample number\n",
    "    # RnF[1] consists of 5652 dictionaries (one per ion), each of these\n",
    "    # dictionaries consists of 43 items (one entry per sample)\n",
    "           \n",
    " # test ReadEdgesFile:\n",
    "    #ReF = ReadEdgesFile(\"/Users/madeleineernst/Documents/PostDoc/Jeremy/edges.txt\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadEdgesFile(EdgesFile, cutoff = 0.65):\n",
    "    \"\"\"\n",
    "    Parses the edge file from GNPS (cosine score)\n",
    "    \"\"\"\n",
    "    with open(EdgesFile, 'r') as F:\n",
    "        PairSet = set({})\n",
    "        Weights = {}\n",
    "        headers = F.readline().split(\"\\t\")\n",
    "        #return headers\n",
    "        for line in F:\n",
    "            data = line.split(\"\\t\")\n",
    "            i = data[0]\n",
    "            j = data[1]\n",
    "            cosine = float(data[4])\n",
    "            if cosine > cutoff:\n",
    "                Weights[(i,j)] = cosine\n",
    "                PairSet.add((i,j))\n",
    "                PairSet.add((j,i))\n",
    "    return (Weights,PairSet)\n",
    "    # the output of ReadEdgesFile consists of a tuple of length 2, ReF[0] is a dictionary of length 7188 (lenght of edge file), \n",
    "    # listing cosine scores of all node connections (names) \n",
    "    # ReF[1] is a set with length 11463, consisting of all possible network node partners?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeSparseMatrix(NameDict,VectorDict):\n",
    "    \n",
    "    \"\"\"\n",
    "    ############\n",
    "    What it does\n",
    "    ############\n",
    "    \n",
    "    Intially Transposes the output of ReadNodes to give a dictionary, SampleVectors\n",
    "    which has the form:\n",
    "        {SampleName:np.array([ion intensities...])}\n",
    "        Where all ion intensities are normailized (summ to 1)\n",
    "        \n",
    "    Then removes all of the zero data to give a sparse (nested dictionary) representation as follows:\n",
    "        {SampleName:{Node1:intensity1, Node2:intensity2, Node5:intensity5 ...}}\n",
    "        In the above representation, absense of Node:intensity data indicates the intensity of the\n",
    "        node in question was zero\n",
    "    \n",
    "    ###############\n",
    "    what it returns\n",
    "    ###############\n",
    "    \n",
    "    SampleVectors - A sparse representation of nodes present in each sample\n",
    "    NodeIDs - A list of IDs for all nodes in the sample\n",
    "    NodeDict - Dictionary of form {NodeID:index in NodeIDs, not sure that this is used for anything}\n",
    "    \"\"\"\n",
    "    \n",
    "    AllZeros = []\n",
    "    NodeIDs = list(VectorDict.keys())\n",
    "    NodeDict = {NodeID:NodeIDs.index(NodeID) for NodeID in NodeIDs}\n",
    "    SampleVectors = {}\n",
    "    \n",
    "    for name in NameDict:\n",
    "        index = NameDict[name]\n",
    "        data = [VectorDict[NodeID][index] for NodeID in NodeIDs]#transpose\n",
    "        SampleVectors[name] = np.array(data)#make it into an array\n",
    "       \n",
    "    for sample in SampleVectors:#sum the ion intensities in the array\n",
    "        TmpVector = np.copy(SampleVectors[sample])\n",
    "        IonSum = sum(TmpVector)\n",
    "        \n",
    "        if IonSum > 0:#divide through by the ion sum to normalize to one\n",
    "            TmpVector = TmpVector/IonSum\n",
    "            NewEntry = {}\n",
    "            \n",
    "            for i in range(len(TmpVector)):#get rid of the zeros and make a sparse representation of the data\n",
    "                if TmpVector[i] > 0:\n",
    "                    NewEntry[NodeIDs[i]] = TmpVector[i]\n",
    "            SampleVectors[sample] = NewEntry\n",
    "                         \n",
    "        else:\n",
    "            SampleVectors[sample] = None\n",
    "            AllZeros.append(sample)#track the things which have no data (all node intensities are zeros)\n",
    "            \n",
    "    for i in AllZeros:\n",
    "        SampleVectors.pop(i,None)\n",
    "        \n",
    "    return(SampleVectors,NodeIDs,NodeDict)\n",
    "    # return a tuple with 3 elements\n",
    "    # MsM[0] has length 43, consisting of 43 dictionaries (one for each sample)\n",
    "\n",
    "    # ReF[1] corresponds to ValidNodes\n",
    "    # this function will be called within MakePairScores, skip this when running \n",
    "    # functions individually and directly run MakePairScores, which implements\n",
    "    # this function on every possible species pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate MS1 intensities as min(A[i],B[j])/max(A[i],B[j]):** <br>\n",
    "\n",
    "output[(i,j)] = min(A[i],B[j])/max(A[i],B[j]) <br>#output[(i,j)] = A[i] * B[j] <br>\n",
    "\n",
    "**or A[i] * B[j]:**   <br>\n",
    "\n",
    "#output[(i,j)] = min(A[i],B[j])/max(A[i],B[j]) <br>\n",
    "output[(i,j)] = A[i] * B[j]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompareSpectralVectors(A,B,ValidNodes):\n",
    "    \"\"\"\n",
    "    Takes two sparse vectors of normalised node intensities (A,B),\n",
    "    performs all against all comparison of entries\n",
    "    and returns a single dictionary (sparse matrix) of intensity similaritres.\n",
    "    Matrix entries are zero (absent from dict) if either Ai or Bj is zero\n",
    "    else min/max. e.g:\n",
    "    \n",
    "       A [1][2]\n",
    "      B\n",
    "     [1]  1 .5\n",
    "     [0]  0  0\n",
    "     \n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    for i in A:\n",
    "        for j in B:\n",
    "            if (i,j) in ValidNodes:\n",
    "                output[(i,j)] = min(A[i],B[j])/max(A[i],B[j])\n",
    "                #output[(i,j)] = A[i] * B[j]\n",
    "                \n",
    "    return output\n",
    "    \n",
    "# SampleVectors = SampleMatrix[0]\n",
    "# MpS = MakePairScores(MsM[0],ReF[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakePairScores(SampleVectors,ValidNodes,limit = None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs all-against-all CompareSpectralVectors for samples (including self-self)\n",
    "    PairScores looks like this:\n",
    "        {(A,B):{(1,1):0.7,(1,2):0.3,(2,1):0.01...},A,C:{(1,1)...}}\n",
    "        A,B,C = sample names\n",
    "        (1,1),(1,2) ... are tuples of nodeIDs\n",
    "        the values are <0.0-1.0. Similarity in intenstities\n",
    "    \"\"\"\n",
    "    \n",
    "    PairScores = {}\n",
    "    samples = list(SampleVectors.keys())\n",
    "    \n",
    "    if limit:\n",
    "        samples = samples[:limit]\n",
    "    StartIndex = 0\n",
    "    \n",
    "    for i in samples:\n",
    "        print(\"...Making comparisons for sample \" + str(i))\n",
    "        for j in samples[StartIndex:]:\n",
    "            PairScores[(i,j)] = CompareSpectralVectors(SampleVectors[i],SampleVectors[j],ValidNodes)\n",
    "            \n",
    "    return PairScores\n",
    "    # returns a dictionary of length 1849 (one entry for each species pair), each dictionary lists then pairwise\n",
    "    # entries of ion intensities between these two samples\n",
    "    ##\n",
    "    # pairscores contains all pairwise single values of min(A[i],B[j])/max(A[i],B[j])\n",
    "    ##\n",
    "                  \n",
    "    # WbC = WeightByCosine(MpS,ReF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeightByCosine(pairs,CosineScores,SelfBonus = 1,OtherPenalty = 1):\n",
    "    \n",
    "    \"\"\"\n",
    "    Takes the sparse matrix dictionary of sample comparisons output by\n",
    "    MakePairScores, and wieghts the entries by cosine similarity of the \n",
    "    two compounds represented at each position\n",
    "    \"\"\" \n",
    "    \n",
    "    for pair in pairs:\n",
    "        for nodes in pairs[pair]:\n",
    "            \n",
    "            #print(nodes)\n",
    "            if nodes[0] == nodes[1]:\n",
    "                pairs[pair][nodes] *= SelfBonus\n",
    "                     \n",
    "            elif nodes in CosineScores:\n",
    "                pairs[pair][nodes] *= CosineScores[nodes]**OtherPenalty\n",
    "                     \n",
    "            else:\n",
    "                pairs[pair][nodes] *= CosineScores[(nodes[1],nodes[0])]**OtherPenalty\n",
    "# this funciton does not return anything\n",
    "        \n",
    "# SP = SumPairs(MpS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SumPairs(pairs):\n",
    "    SummedValues = {}\n",
    "    for pair in pairs:\n",
    "        total = 0\n",
    "        for nodes in pairs[pair]:\n",
    "            total += pairs[pair][nodes]\n",
    "        SummedValues[pair] = total\n",
    "    return SummedValues\n",
    "    # returns dictionary with 1849 entries (each possible species pair) with summed scores per species pair\n",
    "\n",
    "    # NSP = NormaliseSummedPairs(SP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormaliseSummedPairs(SummedPairs):\n",
    "    SelfSet = set({})\n",
    "    for pair in SummedPairs:\n",
    "        if pair[0] == pair[1]:\n",
    "            SelfSet.add(pair)\n",
    "        else:        \n",
    "            A = (pair[0],pair[0])\n",
    "            B = (pair[1],pair[1])\n",
    "            MaxVal = max(SummedPairs[A],SummedPairs[B])\n",
    "            SummedPairs[pair] /= MaxVal\n",
    "    for pair in SelfSet:\n",
    "        SummedPairs[pair] = 1.0\n",
    "        # this function doesn't return anythin, normalizes summed intenisities with max of summed A and B\n",
    "\n",
    "        # FCN = FindClosestNeighbor(SP,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakeDistanceMatrix(SummedPairs,SampleNames,OutFile,ScalingFactor = -1.0, NameMap=None):\n",
    "    \"\"\"\n",
    "    Takes summed pairs, a matrix of similarity scores (higher = more similar)\n",
    "    \n",
    "    Makes an s x s distance matrix comparing all samples to each other\n",
    "    the output can be passed to phylip to use in tree generation\n",
    "    \n",
    "    A 5 x 5 distance matrix to use in phylip looks like this:\n",
    "    \n",
    "        5\n",
    "    Rabbit      0.0000  0.2818  0.9386  0.9830  1.2617\n",
    "    Human       0.2818  0.0000  1.0795  1.1496  1.5312\n",
    "    Opossum     0.9386  1.0795  0.0000  1.5380  1.8615\n",
    "    Chicken     0.9830  1.1496  1.5380  0.0000  1.5819\n",
    "    Frog        1.2617  1.5312  1.8615  1.5819  0.0000\n",
    "    \"\"\"\n",
    "    #make a len(names)**2 zeros matrix\n",
    "    #populate according to name order by adding at appropriate index\n",
    "    #print to file by adding name to each line\n",
    "    matrix = np.zeros((len(SampleNames),len(SampleNames)))\n",
    "    for i in range(len(SampleNames)):\n",
    "        for j in range(len(SampleNames)):\n",
    "            matrix[i,j] += SummedPairs[(SampleNames[i],SampleNames[j])]\n",
    "    \n",
    "    matrix -= 1\n",
    "    matrix *= ScalingFactor\n",
    "    \n",
    "    \"\"\"\n",
    "    if NameMap:\n",
    "        labels = [NameMap[SampleNames[i]] for i in SampleNames]\n",
    "    else:\n",
    "        labels = SampleNames\n",
    "    with open(OutFile,'w') as F:\n",
    "    \"\"\"   \n",
    "    \n",
    "    #file output not implemented yet.\n",
    "    return matrix\n",
    "   \n",
    "def RunAll(edge_file, node_file,Dendro_name,name_map_file,blank_name):\n",
    "    \n",
    "    print(\"reading node data...\")\n",
    "    nodeDat = ReadNodesFile(node_file,blank_name=blank_name)\n",
    "    \n",
    "    print(\"reading edge data...\")\n",
    "    edgeDat = ReadEdgesFile(edge_file)  \n",
    "          \n",
    "    ValidNodes = edgeDat[1]\n",
    "    CosineScores = edgeDat[0]\n",
    "    \n",
    "    print(\"making sparse matrix...\")\n",
    "    SampleMatrix = MakeSparseMatrix(nodeDat[0],nodeDat[1])\n",
    "    \n",
    "    SampleVectors = SampleMatrix[0]\n",
    "    \n",
    "    print(\"Constructing ion abundance similarity matrix: \" )\n",
    "    pairs = MakePairScores(SampleVectors,ValidNodes)\n",
    "    \n",
    "    print(\"weightinng by cosine similarity...\")\n",
    "    WeightByCosine(pairs,CosineScores,SelfBonus = 1,OtherPenalty = 1)\n",
    "    \n",
    "    print(\"Summing similarity matirices...\")\n",
    "    SummedPairs = SumPairs(pairs)\n",
    "    \n",
    "    print(\"Normalising similarity matirices...\") \n",
    "    NormaliseSummedPairs(SummedPairs)\n",
    "    \n",
    "    print(\"parsing name file...\") \n",
    "    names = list(nodeDat[0].keys())\n",
    "    blank_name = None\n",
    "    name_map_file = None\n",
    "    NamesNoBlank = [i  for i in names if i != blank_name]#add capability for multiple blank names...\n",
    "    \n",
    "    if name_map_file:\n",
    "        with open(name_map_file,'rb') as F:\n",
    "            name_map = pickle.load(F)\n",
    "        NamesConverted = [name_map[i] if i in name_map else i for i in NamesNoBlank]\n",
    "    \n",
    "    else:\n",
    "        NamesConverted = NamesNoBlank\n",
    "        \n",
    "    NamesConverted = [SubDict[name] for name in NamesConverted] \n",
    "        \n",
    "    print(\"making dendrogram...\" )    \n",
    "    mat = MakeDistanceMatrix(SummedPairs,NamesNoBlank,\"file\")  \n",
    "    mat = (np.round(mat,decimals = 4))**5\n",
    "    \n",
    "    # save distance matrix with names\n",
    "    df = pd.DataFrame(mat, index=NamesNoBlank, columns=NamesNoBlank)\n",
    "    # save distance matrix with names\n",
    "    df = df.replace(-0.0,0)\n",
    "    df.columns = df.columns.str.replace('_','.')\n",
    "    df.index = df.index.str.replace('_','.')\n",
    "    df.to_csv('NamedDistanceMatrix.txt', index=True, header=True, sep='\\t')\n",
    "    # save distance matrix with names\n",
    "    dists = squareform(mat)\n",
    "    linkage_matrix = linkage(dists, linkage_type)\n",
    "    dendrogram(linkage_matrix, labels=NamesConverted,leaf_font_size=2)\n",
    "    plt.title(\"test\")\n",
    "    plt.savefig(Dendro_name,bbox_inches = 'tight')  \n",
    "    plt.close()   \n",
    "    print(mat)\n",
    "    np.savetxt(\"DitanceMatrix.txt\", mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(edge_file,node_file,Dendro_name):\n",
    "    \n",
    "    \n",
    "        edge_file = edge_file\n",
    "        node_file = node_file\n",
    "        Dendro_name = Dendro_name\n",
    "    \n",
    "\n",
    "        blank_name = None\n",
    "\n",
    "        name_map_file = None  \n",
    "        \n",
    "        RunAll(edge_file, node_file,Dendro_name,name_map_file,blank_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the name of your edges table, feature table and a name for the dendrogram output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading node data...\n",
      "mod1\n",
      "analysing the following samples:\n",
      "***\n",
      "Af-T\n",
      "Af-L\n",
      "Af-F\n",
      "Af-B\n",
      "Ah-L\n",
      "Ahv-B\n",
      "Ah-T\n",
      "Ah-F\n",
      "Ahv-F\n",
      "Ahv-T\n",
      "Ahv-L\n",
      "Aj-B\n",
      "Aj-L\n",
      "Aj-T\n",
      "Aj-F\n",
      "***\n",
      "reading edge data...\n",
      "making sparse matrix...\n",
      "Constructing ion abundance similarity matrix: \n",
      "...Making comparisons for sample Af-T\n",
      "...Making comparisons for sample Af-L\n",
      "...Making comparisons for sample Af-F\n",
      "...Making comparisons for sample Af-B\n",
      "...Making comparisons for sample Ah-L\n",
      "...Making comparisons for sample Ahv-B\n",
      "...Making comparisons for sample Ah-T\n",
      "...Making comparisons for sample Ah-F\n",
      "...Making comparisons for sample Ahv-F\n",
      "...Making comparisons for sample Ahv-T\n",
      "...Making comparisons for sample Ahv-L\n",
      "...Making comparisons for sample Aj-B\n",
      "...Making comparisons for sample Aj-L\n",
      "...Making comparisons for sample Aj-T\n",
      "...Making comparisons for sample Aj-F\n",
      "weightinng by cosine similarity...\n",
      "Summing similarity matirices...\n",
      "Normalising similarity matirices...\n",
      "parsing name file...\n",
      "making dendrogram...\n",
      "[[-0.          0.0218574   0.0261509   0.0023148   0.06171548  0.05886123\n",
      "   0.06683254  0.04630042  0.06059263  0.06456763  0.02965845  0.07106933\n",
      "   0.04103643  0.05661449  0.07959141]\n",
      " [ 0.0218574  -0.          0.04440622  0.02017444  0.03477398  0.11042825\n",
      "   0.10670477  0.05813876  0.10122846  0.12704627  0.02418065  0.10948778\n",
      "   0.04951025  0.0838384   0.08418308]\n",
      " [ 0.0261509   0.04440622 -0.          0.0156474   0.08705237  0.08404507\n",
      "   0.09257778  0.01677465  0.01152922  0.09775612  0.02625947  0.07382458\n",
      "   0.04170133  0.09830207  0.08691062]\n",
      " [ 0.0023148   0.02017444  0.0156474  -0.          0.05280054  0.06139298\n",
      "   0.0879068   0.03433373  0.05626348  0.07532859  0.02716907  0.05686629\n",
      "   0.04061075  0.05399985  0.08963578]\n",
      " [ 0.06171548  0.03477398  0.08705237  0.05280054 -0.          0.01828937\n",
      "   0.0229345   0.08606395  0.05055762  0.02183393  0.01109948  0.02983871\n",
      "   0.04874733  0.0121766   0.05299098]\n",
      " [ 0.05886123  0.11042825  0.08404507  0.06139298  0.01828937 -0.\n",
      "   0.00700969  0.09806779  0.06128578  0.0046856   0.05875758  0.00959148\n",
      "   0.10027183  0.0180867   0.05318198]\n",
      " [ 0.06683254  0.10670477  0.09257778  0.0879068   0.0229345   0.00700969\n",
      "  -0.          0.14830286  0.09861513  0.00837118  0.06957457  0.01772642\n",
      "   0.12513841  0.02308127  0.06857258]\n",
      " [ 0.04630042  0.05813876  0.01677465  0.03433373  0.08606395  0.09806779\n",
      "   0.14830286 -0.          0.03020185  0.13703969  0.0316268   0.08260692\n",
      "   0.03131255  0.06512801  0.0525159 ]\n",
      " [ 0.06059263  0.10122846  0.01152922  0.05626348  0.05055762  0.06128578\n",
      "   0.09861513  0.03020185 -0.          0.08152445  0.03594681  0.07426074\n",
      "   0.02385157  0.04897074  0.06484734]\n",
      " [ 0.06456763  0.12704627  0.09775612  0.07532859  0.02183393  0.0046856\n",
      "   0.00837118  0.13703969  0.08152445 -0.          0.04068788  0.01052471\n",
      "   0.0859937   0.01260899  0.05527015]\n",
      " [ 0.02965845  0.02418065  0.02625947  0.02716907  0.01109948  0.05875758\n",
      "   0.06957457  0.0316268   0.03594681  0.04068788 -0.          0.06828009\n",
      "   0.01236894  0.03805698  0.11293866]\n",
      " [ 0.07106933  0.10948778  0.07382458  0.05686629  0.02983871  0.00959148\n",
      "   0.01772642  0.08260692  0.07426074  0.01052471  0.06828009 -0.\n",
      "   0.07301999  0.01750909  0.04444763]\n",
      " [ 0.04103643  0.04951025  0.04170133  0.04061075  0.04874733  0.10027183\n",
      "   0.12513841  0.03131255  0.02385157  0.0859937   0.01236894  0.07301999\n",
      "  -0.          0.02537441  0.05380651]\n",
      " [ 0.05661449  0.0838384   0.09830207  0.05399985  0.0121766   0.0180867\n",
      "   0.02308127  0.06512801  0.04897074  0.01260899  0.03805698  0.01750909\n",
      "   0.02537441 -0.          0.02983871]\n",
      " [ 0.07959141  0.08418308  0.08691062  0.08963578  0.05299098  0.05318198\n",
      "   0.06857258  0.0525159   0.06484734  0.05527015  0.11293866  0.04444763\n",
      "   0.05380651  0.02983871 -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "main(\"AlnusEdges.txt\",\"Alnus_quant.csv\",\"Dendro_Alnus.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
